{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDL DB BUILD!\n",
    "This notebook is building the roll damping database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyterthemes import jtplot\n",
    "jtplot.style(theme='chesterish', context='notebook', ticks=True, grid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import data\n",
    "import copy\n",
    "from mdldb.mdl_db import MDLDataBase\n",
    "from mdldb.tables import Base, Model, LoadingCondition, Run, RolldecayLinear, RolldecayDirect\n",
    "from mdldb.tables import RolldecayNorwegian, RolldecayDirectImproved, RolldecayLinearAnalytical\n",
    "from mdldb.tables import RolldecayCubic,RolldecayQuadraticBandC,RolldecayQuadraticB,RolldecayLinearB,RolldecaySimplifiedIkeda, RolldecaySimplifiedIkedaUnlimited\n",
    "from mdldb import mdl_to_evaluation\n",
    "from evaluation.run_dynamic import RunDynamic\n",
    "from evaluation.run_manoeuvring import RunZigZag\n",
    "\n",
    "from rolldecayestimators.direct_estimator import DirectEstimator\n",
    "from rolldecayestimators.direct_estimator_improved import DirectEstimatorImproved\n",
    "from rolldecayestimators.direct_estimator_cubic import EstimatorCubic,EstimatorQuadraticB,EstimatorQuadraticBandC,EstimatorLinear\n",
    "from rolldecayestimators.direct_linear_estimator import DirectLinearEstimator\n",
    "from rolldecayestimators.norwegian_estimator import NorwegianEstimator\n",
    "from rolldecayestimators.analytical_linear_estimator import AnalyticalLinearEstimator\n",
    "from rolldecayestimators.ikeda_estimator import IkedaQuadraticEstimator, IkedaEstimatorFitError\n",
    "from rolldecayestimators.simplified_ikeda import SimplifiedIkedaInputError\n",
    "from rolldecayestimators.estimator import FitError\n",
    "\n",
    "from rolldecayestimators.transformers import CutTransformer, LowpassFilterDerivatorTransformer, ScaleFactorTransformer, OffsetTransformer\n",
    "#from rolldecay.equations_lambdify import calculate_acceleration, calculate_velocity\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import signal_lab.mdl_to_evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite:///' + data.mdl_db_path)\n",
    "db = MDLDataBase(engine=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lowpass_filter = LowpassFilterDerivatorTransformer(cutoff=0.4, minimum_score=0.0)\n",
    "lowpass_filter = LowpassFilterDerivatorTransformer(cutoff=2, minimum_score=0.99)\n",
    "scaler = ScaleFactorTransformer(scale_factor=None)  # dummy value None for now\n",
    "cutter = CutTransformer(phi_max=np.deg2rad(9), phi_min=np.deg2rad(0.25), phi1d_start_tolerance=0.015)\n",
    "offset_transformer = OffsetTransformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_linear_estimator = DirectLinearEstimator()\n",
    "\n",
    "steps = [\n",
    "    ('filter',lowpass_filter),\n",
    "    #('scaler',scaler),\n",
    "    ('cutter', cutter), \n",
    "    ('offset_transformer',offset_transformer),\n",
    "    ('linear_estimator', direct_linear_estimator)]\n",
    "        \n",
    "pipeline_direct_linear = Pipeline(steps) # define the pipeline object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear analytical method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytical_linear_estimator = AnalyticalLinearEstimator(omega_regression=True)\n",
    "\n",
    "steps = [\n",
    "    ('filter',lowpass_filter),\n",
    "    #('scaler',scaler),\n",
    "    ('cutter', cutter), \n",
    "    ('offset_transformer',offset_transformer),\n",
    "    ('analytical_linear_estimator', analytical_linear_estimator)]\n",
    "        \n",
    "pipeline_analytical_linear = Pipeline(steps) # define the pipeline object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic direct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_estimator = DirectEstimator(omega_regression=True, fit_method='derivation')\n",
    "#direct_estimator = DirectEstimator(omega_regression=True, fit_method='integration')\n",
    "\n",
    "steps = [\n",
    "    ('filter',lowpass_filter),\n",
    "    #('scaler',scaler),\n",
    "    ('cutter', cutter), \n",
    "    ('offset_transformer',offset_transformer),\n",
    "    ('direct_estimator', direct_estimator)]\n",
    "        \n",
    "pipeline_direct = Pipeline(steps) # define the pipeline object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cubic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_cubic = EstimatorCubic()\n",
    "\n",
    "steps = [\n",
    "    ('filter',lowpass_filter),\n",
    "    #('scaler',scaler),\n",
    "    ('cutter', cutter), \n",
    "    ('offset_transformer',offset_transformer),\n",
    "    ('direct_estimator_cubic', estimator_cubic)]\n",
    "        \n",
    "pipeline_cubic = Pipeline(steps) # define the pipeline object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic B & C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_quadratic_b_and_c = EstimatorQuadraticBandC()\n",
    "\n",
    "steps = [\n",
    "    ('filter',lowpass_filter),\n",
    "    #('scaler',scaler),\n",
    "    ('cutter', cutter), \n",
    "    ('offset_transformer',offset_transformer),\n",
    "    ('direct_estimator_cubic', estimator_quadratic_b_and_c)]\n",
    "        \n",
    "pipeline_quadratic_b_and_c = Pipeline(steps) # define the pipeline object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic B (not C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_quadratic_b = EstimatorQuadraticB()\n",
    "\n",
    "steps = [\n",
    "    ('filter',lowpass_filter),\n",
    "    #('scaler',scaler),\n",
    "    ('cutter', cutter), \n",
    "    ('offset_transformer',offset_transformer),\n",
    "    ('direct_estimator_cubic', estimator_quadratic_b)]\n",
    "        \n",
    "pipeline_quadratic_b = Pipeline(steps) # define the pipeline object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_linear = EstimatorLinear()\n",
    "\n",
    "steps = [\n",
    "    ('filter',lowpass_filter),\n",
    "    #('scaler',scaler),\n",
    "    ('cutter', cutter), \n",
    "    ('offset_transformer',offset_transformer),\n",
    "    ('direct_estimator_cubic', estimator_linear)]\n",
    "        \n",
    "pipeline_linear = Pipeline(steps) # define the pipeline object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified Ikeda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [\n",
    "    ('filter',lowpass_filter),\n",
    "    #('scaler',scaler),\n",
    "    ('cutter', cutter), \n",
    "    ('offset_transformer',offset_transformer),\n",
    "    ]\n",
    "        \n",
    "preprocessor_ikeda = Pipeline(steps) # define the pipeline object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict(db_run, pipeline):\n",
    "    \n",
    "    ascii_file = db_run.load()\n",
    "    df_raw = ascii_file.channels\n",
    "    \n",
    "    df = signal_lab.mdl_to_evaluation.do_transforms(df=df_raw)\n",
    "    \n",
    "    #scaler = _pipline['scaler']\n",
    "    #scaler.scale_factor = db_run.model.scale_factor\n",
    "    \n",
    "    df.rename(columns={'MA/Roll':'phi'}, inplace=True)\n",
    "    \n",
    "    _pipline_derivation = copy.deepcopy(pipeline)\n",
    "    estimator_derivation = _pipline_derivation[-1]\n",
    "    estimator_derivation.set_fit_method(fit_method='derivation')\n",
    "    _pipline_derivation.fit(X=df)\n",
    "\n",
    "    _pipline_integration = copy.deepcopy(pipeline)\n",
    "    estimator_integration = _pipline_integration[-1]\n",
    "    estimator_integration.set_fit_method(fit_method='integration')\n",
    "    \n",
    "    # Take the best of the derivation or integration approach:\n",
    "    try:\n",
    "        _pipline_integration.fit(X=df)\n",
    "    except:\n",
    "        estimator = estimator_derivation\n",
    "        _pipline = _pipline_derivation\n",
    "    else:\n",
    "        if estimator_integration.score() > estimator_derivation.score():\n",
    "            estimator = estimator_integration\n",
    "            _pipline = _pipline_integration\n",
    "        else:\n",
    "            estimator = estimator_derivation\n",
    "            _pipline = _pipline_derivation\n",
    "    \n",
    "    scale_factor=db_run.model.scale_factor\n",
    "    meta_data = {\n",
    "        'Volume':db_run.loading_condition.Volume/(scale_factor**3),\n",
    "        'GM':db_run.loading_condition.gm/scale_factor,         \n",
    "    }\n",
    "    \n",
    "    s = estimator.result_for_database(meta_data=meta_data)\n",
    "    \n",
    "    return s,_pipline, df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict_ikeda(db_run, pipeline, verify_input=True, limit_inputs=True):\n",
    "    \n",
    "    ascii_file = db_run.load()\n",
    "    df_raw = ascii_file.channels\n",
    "    \n",
    "    df = signal_lab.mdl_to_evaluation.do_transforms(df=df_raw)\n",
    "        \n",
    "    _pipline = copy.deepcopy(pipeline)\n",
    "    \n",
    "    #scaler = _pipline['scaler']\n",
    "    #scaler.scale_factor = db_run.model.scale_factor\n",
    "    \n",
    "    df.rename(columns={'MA/Roll':'phi'}, inplace=True)\n",
    "    \n",
    "    # Fit:\n",
    "    _pipline.fit(X=df)\n",
    "    X = _pipline.transform(X=df)\n",
    "    \n",
    "    # Predict:\n",
    "    scale_factor=db_run.model.scale_factor\n",
    "    lpp=db_run.ship.lpp/scale_factor\n",
    "    TA=db_run.loading_condition.TA/scale_factor \n",
    "    TF=db_run.loading_condition.TF/scale_factor\n",
    "    beam=db_run.ship.beam/scale_factor\n",
    "    BKL=db_run.ship.BKL/scale_factor\n",
    "    BKB=db_run.ship.BKB/scale_factor\n",
    "    A0=db_run.loading_condition.A0\n",
    "    kg=db_run.loading_condition.kg/scale_factor\n",
    "    Volume=db_run.loading_condition.Volume/(scale_factor**3)\n",
    "    gm=db_run.loading_condition.gm/scale_factor \n",
    "    V=db_run.ship_speed*1.852/3.6/np.sqrt(scale_factor)  #[m/s]\n",
    "    rho=1000\n",
    "    g=9.81\n",
    "        \n",
    "    estimator = IkedaQuadraticEstimator(lpp=lpp, TA=TA, TF=TF, beam=beam, BKL=BKL, BKB=BKB, A0=A0, \n",
    "                               kg=kg, Volume=Volume, gm=gm, rho=rho, g=g, V=V, \n",
    "                                        verify_input=verify_input, limit_inputs=limit_inputs)\n",
    "    \n",
    "    #estimator.fit(X=X)\n",
    "    estimator.fit(X=X, alternative_bilge_keel=True, RdivB=0.08)  # this give better results.\n",
    "    \n",
    "    s = estimator.result_for_database()\n",
    "    \n",
    "    return s,_pipline, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roll_decay_tests = db.session.query(Run).filter(Run.test_type=='roll decay')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add roll decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#db_run = roll_decay_tests[0]\n",
    "db_run = db.session.query(Run).get(int(6440))\n",
    "\n",
    "s, pipeline, df = fit_predict(db_run=db_run, pipeline=pipeline_cubic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [\n",
    "    ('filter',lowpass_filter),\n",
    "    ('scaler',scaler),\n",
    "    ('cutter', cutter), \n",
    "]\n",
    "\n",
    "preprosessor = Pipeline(steps=steps)\n",
    "preprosessor['scaler'].scale_factor = db_run.model.scale_factor\n",
    "preprosessor.fit(X=df)\n",
    "X = preprosessor.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.plot(y='phi1d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rolldecay_linear_db = RolldecayDirectCubic(run_id=db_run.id,**s)\n",
    "#db.session.merge(rolldecay_linear_db)\n",
    "#db.session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pipeline(pipeline, df):\n",
    "    df_ = df.copy()\n",
    "    df_['n'] = n = np.arange(len(df_))\n",
    "    pipeline = copy.deepcopy(pipeline)\n",
    "    #pipeline['scaler'].scale_factor = db_run.model.scale_factor\n",
    "    \n",
    "    fig,axes = plt.subplots(nrows=len(pipeline))\n",
    "    fig.set_size_inches(15,7)\n",
    "    ax=axes[0]\n",
    "    df_.plot(x='n', y='phi', ax=ax, label='raw')\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    \n",
    "    for step,ax in zip(pipeline[0:-1],axes[1:]):\n",
    "       \n",
    "        step.fit(X=df_)\n",
    "        df_ = step.transform(X=df_)\n",
    "    \n",
    "        df_.plot(x='n', y='phi', label=str(step), ax=ax)\n",
    "        ax.set_xlim(n[0], n[-1])\n",
    "        ax.grid(True)\n",
    "        ax.legend()\n",
    "    \n",
    "    fig,ax = plt.subplots()\n",
    "    fig.set_size_inches(15,5)\n",
    "    estimator = pipeline[-1]\n",
    "    estimator.fit(X=df_)\n",
    "    df_pred = estimator.predict(X=df_)\n",
    "    estimator.plot_fit(ax=ax)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_pipeline(pipeline=pipeline_direct_linear,df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#o = pipeline_direct_linear['offset_transformer']\n",
    "#o.X_zerocrossings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "roll_decay_tests = db.session.query(Run).filter(Run.test_type=='roll decay')\n",
    "\n",
    "pipelines = [\n",
    "    #(pipeline_direct_linear,RolldecayLinear),\n",
    "    #(pipeline_direct,RolldecayDirect),\n",
    "    (pipeline_cubic,RolldecayCubic),\n",
    "    (pipeline_quadratic_b,RolldecayQuadraticB),\n",
    "    (pipeline_quadratic_b_and_c,RolldecayQuadraticBandC),\n",
    "    (pipeline_linear,RolldecayLinearB),\n",
    "    \n",
    "    #(pipeline_norwegian,RolldecayNorwegian),\n",
    "    #(pipeline_direct_improved,RolldecayDirectImproved),\n",
    "    #(pipeline_analytical_linear,RolldecayLinearAnalytical),\n",
    "]\n",
    "\n",
    "skipped=[]\n",
    "\n",
    "for db_run in roll_decay_tests:\n",
    "    \n",
    "    if db_run.ship_speed is None:\n",
    "        db_run.ship_speed=0\n",
    "    \n",
    "    for item in pipelines:  \n",
    "        p = item[0]\n",
    "        DBClass = item[1]\n",
    "        \n",
    "        if db.session.query(DBClass).get(db_run.id):\n",
    "            continue  # This has already been handled\n",
    "        \n",
    "        print('%s run id:%i' % (datetime.now(),db_run.id))\n",
    "        \n",
    "        try:\n",
    "            s, pipeline, df = fit_predict(db_run=db_run, pipeline=p)\n",
    "        except Exception as e:\n",
    "            skipped.append(db_run)\n",
    "            warnings.warn(str(e), Warning)\n",
    "            continue            \n",
    "        else:\n",
    "        \n",
    "            rolldecay_db = DBClass(run_id=db_run.id,**s)\n",
    "            db.session.merge(rolldecay_db)\n",
    "            db.session.commit()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for db_run in skipped:\n",
    "    df = db_run.load().channels\n",
    "    df = signal_lab.mdl_to_evaluation.do_transforms(df=df)\n",
    "    df.rename(columns={'MA/Roll':'phi'}, inplace=True)\n",
    "    \n",
    "    df['phi_deg'] = np.rad2deg(df['phi'])\n",
    "    df.plot(y='phi_deg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipped=[]\n",
    "pipelines = [\n",
    "    (preprocessor_ikeda,RolldecaySimplifiedIkeda),\n",
    "    ]\n",
    "\n",
    "for db_run in roll_decay_tests:\n",
    "    \n",
    "    if db_run.ship_speed is None:\n",
    "        db_run.ship_speed=0\n",
    "    \n",
    "    for item in pipelines:  \n",
    "        p = item[0]\n",
    "        DBClass = item[1]\n",
    "        \n",
    "        if db.session.query(DBClass).get(db_run.id):\n",
    "            continue  # This has already been handled\n",
    "        \n",
    "\n",
    "        try:\n",
    "            s, pipeline, df = fit_predict_ikeda(db_run=db_run, pipeline=p)\n",
    "        except Exception as e:\n",
    "            warnings.warn(str(e), Warning)\n",
    "            \n",
    "            if e is SimplifiedIkedaInputError:\n",
    "                continue\n",
    "            elif e is IkedaEstimatorFitError:\n",
    "                continue\n",
    "            else:\n",
    "                skipped.append(db_run)\n",
    "                continue\n",
    "        else:\n",
    "            rolldecay_db = DBClass(run_id=db_run.id,**s)\n",
    "            db.session.merge(rolldecay_db)\n",
    "            db.session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipped=[]\n",
    "pipelines = [\n",
    "    (preprocessor_ikeda,RolldecaySimplifiedIkedaUnlimited),\n",
    "    ]\n",
    "\n",
    "for db_run in roll_decay_tests:\n",
    "    \n",
    "    if db_run.ship_speed is None:\n",
    "        db_run.ship_speed=0\n",
    "    \n",
    "    for item in pipelines:  \n",
    "        p = item[0]\n",
    "        DBClass = item[1]\n",
    "        \n",
    "        if db.session.query(DBClass).get(db_run.id):\n",
    "            continue  # This has already been handled\n",
    "        \n",
    "\n",
    "        try:\n",
    "            s, pipeline, df = fit_predict_ikeda(db_run=db_run, pipeline=p, verify_input=False, limit_inputs=False)  #Unlimited!!!\n",
    "        except Exception as e:\n",
    "            warnings.warn(str(e), Warning)\n",
    "            \n",
    "            if e is SimplifiedIkedaInputError:\n",
    "                continue\n",
    "            elif e is IkedaEstimatorFitError:\n",
    "                continue\n",
    "            else:\n",
    "                skipped.append(db_run)\n",
    "                continue\n",
    "        else:\n",
    "            rolldecay_db = DBClass(run_id=db_run.id,**s)\n",
    "            db.session.merge(rolldecay_db)\n",
    "            db.session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for db_run in skipped:\n",
    "    df = db_run.load().channels\n",
    "    df = signal_lab.mdl_to_evaluation.do_transforms(df=df)\n",
    "    df.rename(columns={'MA/Roll':'phi'}, inplace=True)\n",
    "    \n",
    "    df['phi_deg'] = np.rad2deg(df['phi'])\n",
    "    df.plot(y='phi_deg')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_run.load().channels\n",
    "df = signal_lab.mdl_to_evaluation.do_transforms(df=df)\n",
    "df.rename(columns={'MA/Roll':'phi'}, inplace=True)\n",
    "\n",
    "df.plot(y='phi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_run.load().channels\n",
    "df = signal_lab.mdl_to_evaluation.do_transforms(df=df)\n",
    "df.rename(columns={'MA/Roll':'phi'}, inplace=True)\n",
    "\n",
    "df.plot(y='phi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [\n",
    "    ('filter',lowpass_filter),\n",
    "    ('scaler',scaler),\n",
    "    ('cutter', cutter), \n",
    "    #('offset_transformer',offset_transformer),\n",
    "]    \n",
    "preprocessor = Pipeline(steps) # define the pipeline object.\n",
    "preprocessor.fit(df)\n",
    "X = preprosessor.transform(df)\n",
    "\n",
    "steps = [\n",
    "    ('filter',lowpass_filter),\n",
    "    ('scaler',scaler),\n",
    "    #('cutter', cutter), \n",
    "    #('offset_transformer',offset_transformer),\n",
    "]    \n",
    "preprocessor2 = Pipeline(steps) # define the pipeline object.\n",
    "preprocessor2.fit(df)\n",
    "X2 = preprocessor2.transform(df)\n",
    "\n",
    "\n",
    "fig,ax=plt.subplots()\n",
    "X2.plot(y='phi',ax=ax)\n",
    "X.plot(y='phi',ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8.5/25-0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
