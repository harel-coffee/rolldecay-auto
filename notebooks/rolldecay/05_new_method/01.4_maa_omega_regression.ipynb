{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roll damping regression\n",
    "Development of an empirical regression method to predict roll damping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyterthemes import jtplot\n",
    "jtplot.style(theme='onedork', context='notebook', ticks=True, grid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 999\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 5\n",
    "\n",
    "import data\n",
    "import copy\n",
    "from rolldecay.bis_system import BisSystem\n",
    "from rolldecay import database\n",
    "from rolldecayestimators.substitute_dynamic_symbols import lambdify,run\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import signal_lab\n",
    "from sqlalchemy.inspection import inspect\n",
    "import seaborn as sns\n",
    "import docs\n",
    "from sympy.parsing.sympy_parser import parse_expr\n",
    "import sympy as sp\n",
    "from rolldecayestimators import symbols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('data.sav')\n",
    "y_s = pd.read_pickle('y.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_key = 'omega0_hat'\n",
    "\n",
    "fig,ax=plt.subplots()\n",
    "y_s[y_key].hist(bins=50, ax=ax)\n",
    "ax.set_title('Historgram: %s' % y_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = np.arange(1,10)\n",
    "degrees = np.arange(1,3)\n",
    "results = pd.DataFrame()\n",
    "\n",
    "variance_treshold = VarianceThreshold(0.0001)\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "y = y_s[y_key]\n",
    "#X = data[important]\n",
    "X=data\n",
    "\n",
    "for k in ks:\n",
    "    for degree in degrees:\n",
    "        select_k_best = SelectKBest(k=k, score_func=f_regression)\n",
    "        \n",
    "       \n",
    "        polynomial_features = PolynomialFeatures(degree=degree)\n",
    "        linear_regression = LinearRegression()\n",
    "        \n",
    "        steps=[\n",
    "            ('polynomial_feature', polynomial_features),\n",
    "            ('standard_scaler', standard_scaler),\n",
    "            ('variance_treshold',variance_treshold),\n",
    "            ('select_k_best',select_k_best),\n",
    "            ('linear_regression', linear_regression)\n",
    "        ]\n",
    "        \n",
    "        model = Pipeline(steps=steps)\n",
    "        score = cross_val_score(estimator=model,X=X,y=y,cv=5).mean()\n",
    "        s = pd.Series()\n",
    "        s['k'] = k\n",
    "        s['degree'] = degree\n",
    "        s['score'] = score\n",
    "        results = results.append(s, ignore_index=True)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values(by='score', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = int(results.iloc[0]['k'])\n",
    "degree = int(results.iloc[0]['degree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_k_best = SelectKBest(k=k, score_func=f_regression)\n",
    "\n",
    "standard_scaler = StandardScaler()\n",
    "polynomial_features = PolynomialFeatures(degree=degree)\n",
    "linear_regression = LinearRegression()\n",
    "\n",
    "steps=[\n",
    "    ('polynomial_feature', polynomial_features),\n",
    "    ('standard_scaler', standard_scaler),\n",
    "    ('variance_treshold',variance_treshold),\n",
    "    ('select_k_best',select_k_best),\n",
    "    #('linear_regression', linear_regression)\n",
    "]\n",
    "\n",
    "preprocessor = Pipeline(steps=steps)\n",
    "\n",
    "steps=[\n",
    "    ('polynomial_feature', polynomial_features),\n",
    "    ('standard_scaler', standard_scaler),\n",
    "    ('variance_treshold',variance_treshold),\n",
    "    ('select_k_best',select_k_best),\n",
    "    #('linear_regression', linear_regression)\n",
    "]\n",
    "\n",
    "model = linear_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = np.array(polynomial_features.get_feature_names())\n",
    "mask_treshold = variance_treshold.get_support()\n",
    "mask_select_k_best = select_k_best.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_index = pd.Series(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_index = feature_names_index[mask_treshold][mask_select_k_best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = polynomial_features.transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X2[:,good_index.index], y, test_size=0.20)\n",
    "linear_regression.fit(X=X_train, y=y_train)\n",
    "\n",
    "score = linear_regression.score(X=X_test, y=y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sympy_symbols = {key:getattr(symbols,key) for key in X.columns}\n",
    "parameters = [sympy_symbols[key] for key in X.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_equations(polynomial_feature:PolynomialFeatures, columns):\n",
    "    \n",
    "    feature_names = np.array(polynomial_feature.get_feature_names())\n",
    "    parameters = [sympy_symbols[key] for key in columns]\n",
    "    \n",
    "    xs={'x%i'%i:name for i,name in enumerate(X.columns)}\n",
    "    xs_sympy={sp.Symbol(key):sympy_symbols[value] for key,value in xs.items()}\n",
    "        \n",
    "    feature_eqs = [1.0,]\n",
    "\n",
    "    for feature in feature_names[1:]:\n",
    "        s_eq = feature\n",
    "    \n",
    "        s_eq = s_eq.replace(' ','*')    \n",
    "        s_eq = s_eq.replace('^','**')    \n",
    "    \n",
    "        sympy_eq = parse_expr(s_eq)\n",
    "        sympy_eq = sympy_eq.subs(xs_sympy)\n",
    "        feature_eqs.append(sympy_eq)\n",
    "        \n",
    "    return np.array(feature_eqs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feture_equations = get_feature_equations(polynomial_feature=polynomial_features, columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_feature_equations = feture_equations[good_index.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhs = 0 \n",
    "for good_feature_equation,coeff in zip(good_feature_equations,linear_regression.coef_):\n",
    "    rhs+=coeff*good_feature_equation\n",
    "rhs+=linear_regression.intercept_\n",
    "\n",
    "omega_hat_regression_eq = sp.Eq(symbols.omega_hat,rhs)\n",
    "omega_hat_regression_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omega_hat_regression_lambda = lambdify(omega_hat_regression_eq.rhs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "dill.settings['recurse'] = True\n",
    "dill.dump(omega_hat_regression_lambda,open('omega0_hat_polynom.sym', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omega_hat = run(omega_hat_regression_lambda,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "ax.plot(model.predict(X2[:,good_index.index]),omega_hat,'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "ax.plot(y_test,model.predict(X_test),'.', alpha=0.5)\n",
    "ax.set_title('Prediction of %s' % y_key)\n",
    "ax.set_xlabel('test: %s' % y_key)\n",
    "ax.set_ylabel('predicted: %s' % y_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'omega0_hat_model.sav'\n",
    "model.fit(X=X, y=y)\n",
    "model.keys=list(X.columns)\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
